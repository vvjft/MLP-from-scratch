{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # only for reading csv\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/15.2M [00:00<?, ?B/s]\n",
      "  7%|▋         | 1.00M/15.2M [00:00<00:09, 1.60MB/s]\n",
      " 20%|█▉        | 3.00M/15.2M [00:00<00:02, 4.61MB/s]\n",
      " 46%|████▌     | 7.00M/15.2M [00:00<00:00, 11.3MB/s]\n",
      " 72%|███████▏  | 11.0M/15.2M [00:01<00:00, 16.6MB/s]\n",
      " 98%|█████████▊| 15.0M/15.2M [00:01<00:00, 21.7MB/s]\n",
      "100%|██████████| 15.2M/15.2M [00:01<00:00, 13.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mnist-in-csv.zip to ./data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d oddrationale/mnist-in-csv -p ./data\n",
    "with zipfile.ZipFile('./data/mnist-in-csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data')\n",
    "os.remove('./data/mnist-in-csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cost and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Activation functions and derivatives with respect to z\"\"\"\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def ReLu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def ReLu_derivative(z):\n",
    "    return np.where(z > 0, 1.0, 0.001)\n",
    "\n",
    "\"\"\"Cost functions and derivatives with respect to a\"\"\"\n",
    "def cross_entropy(y,a):\n",
    "    return np.sum(np.nan_to_num(-y*np.log(a) - (1-y)*np.log(1-a)))\n",
    "\n",
    "def cross_entropy_derivative(y,a):\n",
    "    return np.nan_to_num((a-y) / (a*(1-a)))\n",
    "\n",
    "def quadratic(y,a):\n",
    "    return np.sum(0.5*(y-a)**2)\n",
    "\n",
    "def quadratic_derivative(y,a):\n",
    "    return a-y\n",
    "\n",
    "activation_functions = {'sigmoid': (sigmoid, sigmoid_derivative), 'relu': (ReLu, ReLu_derivative)}\n",
    "cost_functions = {'quadratic': (quadratic, quadratic_derivative), 'cross_entropy': (cross_entropy, cross_entropy_derivative)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, cost_function = 'cross_entropy', activation_function = 'sigmoid'):\n",
    "        self.L = layers\n",
    "        self.W = [np.random.randn(x,y)/np.sqrt(y) for x,y in zip(self.L[1:],self.L[0:-1])] # divide stadard deviation to avoid saturation\n",
    "        self.B = [np.random.randn(x,1) for x in self.L[1:]]\n",
    "\n",
    "        if cost_function in cost_functions:\n",
    "            self.cost_function, self.cost_function_derivative = cost_functions[cost_function]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid cost function: {cost_function}\")\n",
    "        if activation_function in activation_functions:\n",
    "            self.activation_function, self.activation_derivative = activation_functions[activation_function]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function: {activation_function}\")\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        A = X\n",
    "        for w, b in zip(self.W, self.B):\n",
    "            Z = np.dot(w, A) + b\n",
    "            A = self.activation_function(Z)\n",
    "        return A\n",
    "\n",
    "    def fit(self, train_set, batch_size, epochs, eta, lmbda, patience=10, valid_set=None):\n",
    "        # Set-up\n",
    "        X_train, Y_train = train_set[0], train_set[1]\n",
    "        if valid_set is not None:\n",
    "            X_valid, Y_valid = valid_set[0], valid_set[1]\n",
    "            print(\"Tracking progress on the validation set:\")\n",
    "        else:\n",
    "            print(\"Tracking progress on the training set:\")\n",
    "        best_acc, lowest_cost, no_progress_count = 0.0, np.Inf, 0\n",
    "\n",
    "        # Training     \n",
    "        for epoch in range(epochs):\n",
    "            X_batches = np.array_split(X_train, X_train.shape[1] // batch_size, axis=1)\n",
    "            Y_batches = np.array_split(Y_train, Y_train.shape[1] // batch_size, axis=1)\n",
    "            for X_batch, Y_batch in zip(X_batches, Y_batches):  \n",
    "                nabla_B = [np.zeros(b.shape) for b in self.B]\n",
    "                nabla_W = [np.zeros(w.shape) for w in self.W]\n",
    "                for i in range(X_batch.shape[1]):\n",
    "                    a = X_batch[:,i].reshape(-1,1)\n",
    "                    y = Y_batch[:,i].reshape(-1,1)\n",
    "                    W_shifts, B_shifts = self.get_shifts(a, y)\n",
    "                    nabla_B = [nb+dnb for nb, dnb in zip(nabla_B, B_shifts)] \n",
    "                    nabla_W = [nw+dnw for nw, dnw in zip(nabla_W, W_shifts)]\n",
    "                self.W = [w-eta*nw/X_batch.shape[1] - eta*lmbda*w/X_train.shape[1] for w, nw in zip(self.W, nabla_W)] # L2 regularization\n",
    "                self.B = [b-eta*nb/X_batch.shape[1] for b, nb in zip(self.B, nabla_B)]\n",
    "\n",
    "            # Tracking progress\n",
    "            if valid_set is not  None:\n",
    "                acc, cost = self.__track_progress(X_valid, Y_valid) \n",
    "            else:\n",
    "                acc, cost = self.__track_progress(X_train, Y_train)\n",
    "\n",
    "            if acc > best_acc or cost < lowest_cost:\n",
    "                no_progress_count=0\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                else:\n",
    "                    no_progress_count += 1\n",
    "                if cost < lowest_cost:\n",
    "                    lowest_cost = cost      \n",
    "                else:\n",
    "                    no_progress_count += 2\n",
    "                best_W, best_B = self.W, self.B\n",
    "            else:\n",
    "                no_progress_count += 3\n",
    "            if no_progress_count > patience:\n",
    "                self.W, self.B = best_W, best_B\n",
    "                print(f\"Early stopping: no improvement on validation set for {patience} epochs. Saving parameters from epoch {epoch-patience}.\")\n",
    "                break\n",
    "            else:    \n",
    "                print(f\"epoch: {epoch}, ACC: {acc}, cost: {cost}\")\n",
    "                \n",
    "    def __track_progress(self, X, Y):\n",
    "        \"\"\"\n",
    "        Evaluates accuracy and cost and the end of each epoch.\n",
    "        \"\"\"\n",
    "        acc = self.evaluate(X, Y)[1]\n",
    "        cost = self.cost_function(Y, self.feedforward(X))\n",
    "        return acc, cost\n",
    "\n",
    "    def get_shifts(self, a, y):\n",
    "        \"\"\"\n",
    "        Updates network's weights and biases by applying SGD and backpropagation.\n",
    "        \"\"\"\n",
    "        Z=[]\n",
    "        A=[a]\n",
    "        for w,b in zip(self.W,self.B):\n",
    "            z = np.dot(w,A[-1])+b\n",
    "            a=self.activation_function(z)\n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "        return self.__backprob(y, A, Z)\n",
    "\n",
    "    def __backprob(self,y,A,Z):\n",
    "        def delta(y,x,z):\n",
    "            return self.cost_function_derivative(y,x)*self.activation_derivative(z)\n",
    "            \n",
    "        D = [delta(y,A[-1],Z[-1])] # eq. (1)\n",
    "        for i in range(1,len(Z)):\n",
    "            D.insert(0, np.dot(self.W[-i].T,D[0])*self.activation_derivative(Z[-i-1])) # eq. (2)\n",
    "        B_shifts = D # eq. (3)\n",
    "        W_shifts = []\n",
    "        for a,d in zip(A[0:-1],D):\n",
    "            W_shifts.append(np.dot(d,a.T)) # eq. (4)\n",
    "        return W_shifts, B_shifts\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        correct_predictions = 0\n",
    "        predictions = self.feedforward(X_test)\n",
    "        for prediction, y in zip(predictions.T, Y_test.T):\n",
    "            if np.argmax(prediction) == np.argmax(y):\n",
    "                correct_predictions += 1\n",
    "        return correct_predictions, correct_predictions/(X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\mnist_train.csv').to_numpy()\n",
    "test = pd.read_csv('data\\mnist_test.csv').to_numpy()\n",
    "\n",
    "# Normalize data\n",
    "X_train, Y_train = train[:,1:] / 255 , train[:,0]\n",
    "X_test, Y_test = test[:,1:] / 255, test[:,0] \n",
    "\n",
    "# Shuffle training data\n",
    "perm = np.random.permutation(len(X_train))\n",
    "X_train = X_train[perm].T\n",
    "Y_train = Y_train[perm]\n",
    "\n",
    "# Divide to training, test and validation\n",
    "X_valid = X_train[:,:10000]\n",
    "X_train = X_train[:,10000:]\n",
    "X_test=X_test.T\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "# Example: 2 -> (0,0,1,0,0,0,0,0,0,0) \n",
    "Y_valid = Y_train[:10000]\n",
    "Y_train = Y_train[10000:]\n",
    "Y_train = np.eye(10)[Y_train].T\n",
    "Y_valid = np.eye(10)[Y_valid].T\n",
    "Y_test = np.eye(10)[Y_test].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784,30,10], cost_function='cross_entropy', activation_function='sigmoid')\n",
    "train = [X_train, Y_train]\n",
    "valid = [X_valid, Y_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking progress on the validation set:\n",
      "epoch: 0, ACC: 0.9188, cost: 5410.0647749628115\n",
      "epoch: 1, ACC: 0.9344, cost: 4509.51900163403\n",
      "epoch: 2, ACC: 0.9401, cost: 4108.828540293254\n",
      "epoch: 3, ACC: 0.9457, cost: 3855.323878866133\n",
      "epoch: 4, ACC: 0.9492, cost: 3672.585495175106\n",
      "epoch: 5, ACC: 0.9505, cost: 3542.1775510942753\n",
      "epoch: 6, ACC: 0.953, cost: 3447.5979567402837\n",
      "epoch: 7, ACC: 0.9534, cost: 3376.7663727253394\n",
      "epoch: 8, ACC: 0.9541, cost: 3321.449788199254\n",
      "epoch: 9, ACC: 0.9553, cost: 3275.6341125728695\n",
      "epoch: 10, ACC: 0.9561, cost: 3235.297266063156\n",
      "epoch: 11, ACC: 0.9568, cost: 3198.79388253256\n",
      "epoch: 12, ACC: 0.9569, cost: 3165.850955168115\n",
      "epoch: 13, ACC: 0.9573, cost: 3136.293984138369\n",
      "epoch: 14, ACC: 0.9578, cost: 3109.8177461562163\n",
      "epoch: 15, ACC: 0.9581, cost: 3085.9961343052196\n",
      "epoch: 16, ACC: 0.959, cost: 3064.3320386105465\n",
      "epoch: 17, ACC: 0.9593, cost: 3044.391173531185\n",
      "epoch: 18, ACC: 0.9597, cost: 3025.871597070542\n",
      "epoch: 19, ACC: 0.96, cost: 3008.5841576154257\n",
      "epoch: 20, ACC: 0.9598, cost: 2992.408044483751\n",
      "epoch: 21, ACC: 0.9597, cost: 2977.256390892911\n",
      "epoch: 22, ACC: 0.9602, cost: 2963.0596219199992\n",
      "epoch: 23, ACC: 0.96, cost: 2949.7644876256095\n",
      "epoch: 24, ACC: 0.9599, cost: 2937.331946811409\n",
      "epoch: 25, ACC: 0.96, cost: 2925.7288468144816\n",
      "epoch: 26, ACC: 0.9599, cost: 2914.9189284211566\n",
      "epoch: 27, ACC: 0.96, cost: 2904.859780382516\n",
      "epoch: 28, ACC: 0.96, cost: 2895.5080825474283\n",
      "epoch: 29, ACC: 0.9601, cost: 2886.8255202071837\n",
      "epoch: 30, ACC: 0.96, cost: 2878.7778618433217\n",
      "epoch: 31, ACC: 0.96, cost: 2871.3289282952\n",
      "epoch: 32, ACC: 0.9602, cost: 2864.4358583502426\n",
      "epoch: 33, ACC: 0.96, cost: 2858.04929536219\n",
      "epoch: 34, ACC: 0.9603, cost: 2852.117443729776\n",
      "epoch: 35, ACC: 0.9604, cost: 2846.590881412246\n",
      "epoch: 36, ACC: 0.9609, cost: 2841.42584480157\n",
      "epoch: 37, ACC: 0.9611, cost: 2836.5855568570914\n",
      "epoch: 38, ACC: 0.9614, cost: 2832.040281690085\n",
      "epoch: 39, ACC: 0.9615, cost: 2827.7666404666147\n",
      "epoch: 40, ACC: 0.9617, cost: 2823.7462606196914\n",
      "epoch: 41, ACC: 0.9614, cost: 2819.9639799784604\n",
      "epoch: 42, ACC: 0.9615, cost: 2816.406177560161\n",
      "epoch: 43, ACC: 0.9617, cost: 2813.0596689701338\n",
      "epoch: 44, ACC: 0.9619, cost: 2809.9112017109446\n",
      "epoch: 45, ACC: 0.9618, cost: 2806.947374915598\n",
      "epoch: 46, ACC: 0.9617, cost: 2804.15481172419\n",
      "epoch: 47, ACC: 0.9619, cost: 2801.5204567921305\n",
      "epoch: 48, ACC: 0.9619, cost: 2799.031886731943\n",
      "epoch: 49, ACC: 0.9621, cost: 2796.677534687333\n",
      "epoch: 50, ACC: 0.9624, cost: 2794.446771542267\n",
      "epoch: 51, ACC: 0.9623, cost: 2792.3298465722655\n",
      "epoch: 52, ACC: 0.9623, cost: 2790.317738058288\n",
      "epoch: 53, ACC: 0.9625, cost: 2788.401980581152\n",
      "epoch: 54, ACC: 0.9623, cost: 2786.5745266397744\n",
      "epoch: 55, ACC: 0.9624, cost: 2784.827681492349\n",
      "epoch: 56, ACC: 0.9624, cost: 2783.154130925286\n",
      "epoch: 57, ACC: 0.9624, cost: 2781.5470657987785\n",
      "epoch: 58, ACC: 0.9624, cost: 2780.0003978282843\n",
      "epoch: 59, ACC: 0.9626, cost: 2778.509055690014\n",
      "epoch: 60, ACC: 0.9625, cost: 2777.0693358189756\n",
      "epoch: 61, ACC: 0.962, cost: 2775.6792440306467\n",
      "epoch: 62, ACC: 0.9621, cost: 2774.338708830002\n",
      "epoch: 63, ACC: 0.9621, cost: 2773.0495119664038\n",
      "epoch: 64, ACC: 0.9618, cost: 2771.814813580809\n",
      "epoch: 65, ACC: 0.9621, cost: 2770.638264822531\n",
      "epoch: 66, ACC: 0.9624, cost: 2769.5228659432296\n",
      "epoch: 67, ACC: 0.9625, cost: 2768.4698760414694\n",
      "epoch: 68, ACC: 0.9626, cost: 2767.478131413333\n",
      "epoch: 69, ACC: 0.9628, cost: 2766.5440118535303\n",
      "epoch: 70, ACC: 0.9628, cost: 2765.6620314641004\n",
      "epoch: 71, ACC: 0.9627, cost: 2764.8257810672253\n",
      "epoch: 72, ACC: 0.9629, cost: 2764.0288668462963\n",
      "epoch: 73, ACC: 0.963, cost: 2763.265577315905\n",
      "epoch: 74, ACC: 0.9631, cost: 2762.531169203889\n",
      "epoch: 75, ACC: 0.9627, cost: 2761.821811490057\n",
      "epoch: 76, ACC: 0.9627, cost: 2761.1343277778724\n",
      "epoch: 77, ACC: 0.9627, cost: 2760.4659062269398\n",
      "epoch: 78, ACC: 0.9626, cost: 2759.813900292505\n",
      "epoch: 79, ACC: 0.9625, cost: 2759.1757596092875\n",
      "epoch: 80, ACC: 0.9624, cost: 2758.549060826815\n",
      "epoch: 81, ACC: 0.9625, cost: 2757.931578888478\n",
      "epoch: 82, ACC: 0.9624, cost: 2757.321344508413\n",
      "epoch: 83, ACC: 0.9626, cost: 2756.7166597016844\n",
      "epoch: 84, ACC: 0.9621, cost: 2756.1160802883437\n",
      "epoch: 85, ACC: 0.962, cost: 2755.5184065578437\n",
      "epoch: 86, ACC: 0.962, cost: 2754.9227252487194\n",
      "epoch: 87, ACC: 0.9621, cost: 2754.3285083146043\n",
      "epoch: 88, ACC: 0.962, cost: 2753.735725505896\n",
      "epoch: 89, ACC: 0.962, cost: 2753.1449067008175\n",
      "epoch: 90, ACC: 0.9618, cost: 2752.5571053697513\n",
      "epoch: 91, ACC: 0.9618, cost: 2751.973753318746\n",
      "epoch: 92, ACC: 0.962, cost: 2751.39643864951\n",
      "epoch: 93, ACC: 0.9619, cost: 2750.8266629399445\n",
      "epoch: 94, ACC: 0.9619, cost: 2750.265632709372\n",
      "epoch: 95, ACC: 0.9618, cost: 2749.714122857088\n",
      "epoch: 96, ACC: 0.9618, cost: 2749.1724266226565\n",
      "epoch: 97, ACC: 0.9617, cost: 2748.640383904751\n",
      "epoch: 98, ACC: 0.9617, cost: 2748.117462964853\n",
      "epoch: 99, ACC: 0.9619, cost: 2747.602864687874\n",
      "epoch: 100, ACC: 0.9619, cost: 2747.095623799743\n",
      "epoch: 101, ACC: 0.962, cost: 2746.5946923675738\n",
      "epoch: 102, ACC: 0.9619, cost: 2746.0990012748653\n",
      "epoch: 103, ACC: 0.962, cost: 2745.607501941849\n",
      "epoch: 104, ACC: 0.962, cost: 2745.119193306073\n",
      "epoch: 105, ACC: 0.962, cost: 2744.6331393547807\n",
      "epoch: 106, ACC: 0.9619, cost: 2744.148481636472\n",
      "epoch: 107, ACC: 0.9619, cost: 2743.664449964419\n",
      "epoch: 108, ACC: 0.9616, cost: 2743.180373271923\n",
      "epoch: 109, ACC: 0.9616, cost: 2742.695691326412\n",
      "epoch: 110, ACC: 0.9617, cost: 2742.209966718602\n",
      "epoch: 111, ACC: 0.9618, cost: 2741.722895277252\n",
      "epoch: 112, ACC: 0.9618, cost: 2741.2343121609492\n",
      "epoch: 113, ACC: 0.9617, cost: 2740.7441910344587\n",
      "epoch: 114, ACC: 0.9617, cost: 2740.2526356933977\n",
      "epoch: 115, ACC: 0.9617, cost: 2739.759867171771\n",
      "epoch: 116, ACC: 0.9615, cost: 2739.2662128312127\n",
      "epoch: 117, ACC: 0.9614, cost: 2738.7721039551843\n",
      "epoch: 118, ACC: 0.9614, cost: 2738.278083589803\n",
      "epoch: 119, ACC: 0.9615, cost: 2737.784820118643\n",
      "epoch: 120, ACC: 0.9615, cost: 2737.2931193996283\n",
      "epoch: 121, ACC: 0.9616, cost: 2736.8039291591845\n",
      "epoch: 122, ACC: 0.9618, cost: 2736.3183282117748\n",
      "epoch: 123, ACC: 0.9618, cost: 2735.837487781463\n",
      "epoch: 124, ACC: 0.9619, cost: 2735.362589528413\n",
      "epoch: 125, ACC: 0.9619, cost: 2734.8946938422123\n",
      "epoch: 126, ACC: 0.962, cost: 2734.4345726652587\n",
      "epoch: 127, ACC: 0.9619, cost: 2733.9825431255003\n",
      "epoch: 128, ACC: 0.9619, cost: 2733.538349755064\n",
      "epoch: 129, ACC: 0.962, cost: 2733.1011384450076\n",
      "epoch: 130, ACC: 0.9622, cost: 2732.6695441264274\n",
      "epoch: 131, ACC: 0.9623, cost: 2732.2418791869927\n",
      "epoch: 132, ACC: 0.9622, cost: 2731.816370855943\n",
      "epoch: 133, ACC: 0.9623, cost: 2731.3913730124536\n",
      "epoch: 134, ACC: 0.9623, cost: 2730.9654890041425\n",
      "epoch: 135, ACC: 0.9624, cost: 2730.5375841855166\n",
      "epoch: 136, ACC: 0.9627, cost: 2730.1067139877146\n",
      "epoch: 137, ACC: 0.9628, cost: 2729.672018032546\n",
      "epoch: 138, ACC: 0.9628, cost: 2729.2326260983186\n",
      "epoch: 139, ACC: 0.9627, cost: 2728.78760035134\n",
      "epoch: 140, ACC: 0.9627, cost: 2728.3359166826776\n",
      "epoch: 141, ACC: 0.9626, cost: 2727.876474936035\n",
      "epoch: 142, ACC: 0.9625, cost: 2727.4081236522447\n",
      "epoch: 143, ACC: 0.9626, cost: 2726.9296864038984\n",
      "epoch: 144, ACC: 0.9626, cost: 2726.43998061503\n",
      "epoch: 145, ACC: 0.9626, cost: 2725.937823991199\n",
      "epoch: 146, ACC: 0.9625, cost: 2725.4220274041772\n",
      "epoch: 147, ACC: 0.9625, cost: 2724.8913758706785\n",
      "epoch: 148, ACC: 0.9624, cost: 2724.344600919324\n",
      "epoch: 149, ACC: 0.9623, cost: 2723.7803480940506\n",
      "epoch: 150, ACC: 0.9623, cost: 2723.197142778893\n",
      "epoch: 151, ACC: 0.9623, cost: 2722.593356411206\n",
      "epoch: 152, ACC: 0.9622, cost: 2721.9671741263546\n",
      "epoch: 153, ACC: 0.9624, cost: 2721.3165646161247\n",
      "epoch: 154, ACC: 0.9625, cost: 2720.6392540874695\n",
      "epoch: 155, ACC: 0.9625, cost: 2719.9327093545285\n",
      "epoch: 156, ACC: 0.9624, cost: 2719.194141540923\n",
      "epoch: 157, ACC: 0.9624, cost: 2718.4205542355617\n",
      "epoch: 158, ACC: 0.9624, cost: 2717.608882149679\n",
      "epoch: 159, ACC: 0.9624, cost: 2716.7562981997667\n",
      "epoch: 160, ACC: 0.9624, cost: 2715.860785647001\n",
      "epoch: 161, ACC: 0.9623, cost: 2714.922006491519\n",
      "epoch: 162, ACC: 0.9624, cost: 2713.9422650485167\n",
      "epoch: 163, ACC: 0.9625, cost: 2712.9270606957657\n",
      "epoch: 164, ACC: 0.9627, cost: 2711.884732169181\n",
      "epoch: 165, ACC: 0.9627, cost: 2710.8252299228507\n",
      "epoch: 166, ACC: 0.9627, cost: 2709.758623380756\n",
      "epoch: 167, ACC: 0.9627, cost: 2708.6939648140833\n",
      "epoch: 168, ACC: 0.9628, cost: 2707.6387243564113\n",
      "epoch: 169, ACC: 0.9629, cost: 2706.5986695659312\n",
      "epoch: 170, ACC: 0.9629, cost: 2705.5779733277764\n",
      "epoch: 171, ACC: 0.963, cost: 2704.579392763744\n",
      "epoch: 172, ACC: 0.963, cost: 2703.6044439118614\n",
      "epoch: 173, ACC: 0.9631, cost: 2702.6535517476395\n",
      "epoch: 174, ACC: 0.9629, cost: 2701.7261820056356\n",
      "epoch: 175, ACC: 0.963, cost: 2700.8209737137504\n",
      "epoch: 176, ACC: 0.9632, cost: 2699.9358983238762\n",
      "epoch: 177, ACC: 0.9632, cost: 2699.0684710761916\n",
      "epoch: 178, ACC: 0.9635, cost: 2698.2160237949724\n",
      "epoch: 179, ACC: 0.9636, cost: 2697.3760122977014\n",
      "epoch: 180, ACC: 0.9636, cost: 2696.5462924795393\n",
      "epoch: 181, ACC: 0.9636, cost: 2695.7252894457147\n",
      "epoch: 182, ACC: 0.9636, cost: 2694.9120222672555\n",
      "epoch: 183, ACC: 0.9636, cost: 2694.106010798592\n",
      "epoch: 184, ACC: 0.9639, cost: 2693.307133946334\n",
      "epoch: 185, ACC: 0.9639, cost: 2692.5155054909183\n",
      "epoch: 186, ACC: 0.964, cost: 2691.731399096645\n",
      "epoch: 187, ACC: 0.9639, cost: 2690.955219625179\n",
      "epoch: 188, ACC: 0.964, cost: 2690.18750079025\n",
      "epoch: 189, ACC: 0.964, cost: 2689.428908340909\n",
      "epoch: 190, ACC: 0.964, cost: 2688.6802347104726\n",
      "epoch: 191, ACC: 0.964, cost: 2687.9423787641663\n",
      "epoch: 192, ACC: 0.9639, cost: 2687.216310276743\n",
      "epoch: 193, ACC: 0.9638, cost: 2686.503022797978\n",
      "epoch: 194, ACC: 0.9639, cost: 2685.8034809771834\n",
      "epoch: 195, ACC: 0.9639, cost: 2685.1185694469455\n",
      "epoch: 196, ACC: 0.964, cost: 2684.449050061719\n",
      "epoch: 197, ACC: 0.9638, cost: 2683.795532731475\n",
      "epoch: 198, ACC: 0.9639, cost: 2683.1584625720534\n",
      "epoch: 199, ACC: 0.964, cost: 2682.538123126686\n"
     ]
    }
   ],
   "source": [
    "net.fit(train_set=train, batch_size=10, epochs=200, eta=0.1, lmbda=5.0, patience=10, valid_set=valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9637, 0.9637)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.evaluate(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
